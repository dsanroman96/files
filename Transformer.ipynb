{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAxwnct2QLNsLRxyaQ0ybe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsanroman96/files/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zjr4-O6A1ag"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "from tensorflow.keras.layers import Dense, Embedding, BatchNormalization, LayerNormalization\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OqyPCQ7FbOH"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMx9UnPtAreX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9aa125e-6343-4812-9043-7e81be4d198e"
      },
      "source": [
        "raw_data = (\n",
        "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
        "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
        "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
        "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
        "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
        "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
        "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
        "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
        "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
        "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
        "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
        "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
        "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
        "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
        "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
        "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
        "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
        "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
        "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
        "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
        ")\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "\n",
        "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
        "raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)\n",
        "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
        "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
        "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]\n",
        "\n",
        "\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
        "                                                        padding='post')\n",
        "\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
        "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
        "                                                            padding='post')\n",
        "\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(20).batch(BATCH_SIZE)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 10), (None, 14), (None, 14)), types: (tf.int32, tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7CZmCch_ro3"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTnwcnqX_kfv"
      },
      "source": [
        "def positional_embedding(max_len, model_size):\n",
        "  embedding = []\n",
        "  for pos in range(max_len):\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "      if i %2 == 0:\n",
        "        PE[:,i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "      else:\n",
        "        PE[:,i] = np.cos(pos / 10000 ** ((i-1) / model_size))\n",
        "    embedding.append(PE)\n",
        "  \n",
        "  embedding=np.concatenate(embedding, axis=0)\n",
        "  embedding = tf.constant(embedding, dtype=tf.float32)\n",
        "  return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVBoE9zCArkf"
      },
      "source": [
        "## Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kSSmRlwAt6x"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.Model):\n",
        "  def __init__(self, model_size, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.heads_size = model_size // num_heads\n",
        "    self.querys = Dense(model_size)\n",
        "    self.keys = Dense(model_size)\n",
        "    self.values = Dense(model_size)\n",
        "    self.output_layer = Dense(model_size)\n",
        "  \n",
        "  def create_head(self, head, batch_size):\n",
        "    head = tf.reshape(head, [batch_size, -1, self.num_heads, self.heads_size])\n",
        "    head = tf.transpose(head, [0, 2, 1, 3])\n",
        "    return head\n",
        "\n",
        "  def call(self, target_seq, input_seq, mask=None):\n",
        "    q = self.querys(target_seq)\n",
        "    k = self.keys(input_seq)\n",
        "    v = self.values(input_seq)\n",
        "\n",
        "    batch_size = q.shape[0]\n",
        "    q = self.create_head(q, batch_size)\n",
        "    k = self.create_head(k, batch_size)\n",
        "    v = self.create_head(v, batch_size)\n",
        "\n",
        "    score = tf.matmul(q, k, transpose_b=True)\n",
        "    score /= tf.math.sqrt(tf.dtypes.cast(self.heads_size, dtype=tf.float32))\n",
        "\n",
        "    if mask is not None:\n",
        "      score *= mask\n",
        "      score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "\n",
        "    alignment = tf.nn.softmax(score, axis=-1)\n",
        "    context = tf.matmul(alignment, v)\n",
        "    context = tf.transpose(context, [0, 2, 1, 3])\n",
        "    context = tf.reshape(context, [batch_size, -1, self.heads_size * self.num_heads])\n",
        "\n",
        "    heads = self.output_layer(context)\n",
        "\n",
        "    return heads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf96BKFf8Fdv"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf9Gc3_fP6WO"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, model_size, num_layers, num_heads, pes):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.model_size = model_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.pes = pes\n",
        "    \n",
        "    # One Embedding Layer\n",
        "    self.embedding = Embedding(vocab_size, model_size)\n",
        "\n",
        "    # Multi-Head Attention and Normalization layers\n",
        "    self.attention = [MultiHeadAttention(model_size, num_heads) for _ in range(num_layers)]\n",
        "    #self.attention_norm = [BatchNormalization() for _ in range(num_layers)]\n",
        "    self.attention_norm = [LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "    self.dense_1 = [Dense(model_size * 4, activation=\"relu\") for _ in range(num_layers)]\n",
        "    self.dense_2 = [Dense(model_size) for _ in range(num_layers)]\n",
        "    self.dense_norm = [LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "  def call(self, sequence, padding_mask=None):\n",
        "    embed = self.embedding(sequence)\n",
        "    embed += self.pes[:sequence.shape[1], :]\n",
        "    \n",
        "    sub_in = embed\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      sub_out = self.attention[i](sub_in, sub_in, mask=None)\n",
        "      sub_out = sub_in + sub_out\n",
        "      sub_out = self.attention_norm[i](sub_out)\n",
        "\n",
        "      feed_forward = sub_out\n",
        "\n",
        "      feed_forward = self.dense_1[i](feed_forward)\n",
        "      feed_forward = self.dense_2[i](feed_forward)\n",
        "      feed_forward = feed_forward + sub_out\n",
        "      feed_forward = self.dense_norm[i](feed_forward)\n",
        "\n",
        "      # Assign the Feed Forward Network output to the next layer's Multi-Head Attention input\n",
        "      sub_in = feed_forward\n",
        "    \n",
        "    return feed_forward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tq-_5Ht8Hxl"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCzvef4ozoEM"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, model_size, num_layers, num_heads, pes):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.model_size = model_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.pes = pes\n",
        "    self.embedding = Embedding(vocab_size, model_size)\n",
        "    \n",
        "    self.attention_1 = [MultiHeadAttention(model_size, num_heads) for _ in range(num_layers)]\n",
        "    self.attention_norm_1 = [LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "    self.attention_2 = [MultiHeadAttention(model_size, num_heads) for _ in range(num_layers)]\n",
        "    self.attention_norm_2 = [LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "    self.dense_1 = [Dense(model_size * 4, activation=\"relu\") for _ in range(num_layers)]\n",
        "    self.dense_2 = [Dense(model_size) for _ in range(num_layers)]\n",
        "    self.dense_norm = [LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "    self.dense_out = Dense(vocab_size)\n",
        "\n",
        "  def call(self, sequence, encoder_output, padding_mask=None):\n",
        "    embed = self.embedding(sequence)\n",
        "    embed += self.pes[:sequence.shape[1], :]\n",
        "    sub_in_1 = embed\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "\n",
        "      look_left_only_mask = tf.linalg.band_part(tf.ones((sequence.shape[1], sequence.shape[1])), -1, 0)\n",
        "      sub_out_1 = self.attention_1[i](sub_in_1, sub_in_1, look_left_only_mask)\n",
        "      sub_out_1 = sub_out_1 + sub_in_1\n",
        "      sub_out_1 = self.attention_norm_1[i](sub_out_1)\n",
        "\n",
        "      sub_in_2 = sub_out_1\n",
        "\n",
        "      sub_out_2 = self.attention_2[i](sub_in_2, encoder_output, padding_mask)\n",
        "      sub_out_2 = sub_out_2 + sub_in_2\n",
        "      sub_out_2 = self.attention_norm_2[i](sub_out_2)\n",
        "      \n",
        "      feed_forward = sub_out_2\n",
        "\n",
        "      feed_forward = self.dense_1[i](feed_forward)\n",
        "      feed_forward = self.dense_2[i](feed_forward)\n",
        "      feed_forward = feed_forward + sub_out_2\n",
        "      feed_forward = self.dense_norm[i](feed_forward)\n",
        "\n",
        "      sub_in_1 = feed_forward\n",
        "\n",
        "    logits = self.dense_out(feed_forward)\n",
        "    return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32R_1V28OMB"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT3zfw7C5Rlm"
      },
      "source": [
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 2\n",
        "MODEL_SIZE = 64\n",
        "MAX_LEN = max(len(data_en[0]), len(data_fr_in[0]))\n",
        "\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "pes = positional_embedding(MAX_LEN, MODEL_SIZE)\n",
        "\n",
        "encoder = Encoder(en_vocab_size, MODEL_SIZE, NUM_LAYERS, NUM_HEADS, pes)\n",
        "decoder = Decoder(fr_vocab_size, MODEL_SIZE, NUM_LAYERS, NUM_HEADS, pes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeioGfVHMP4d"
      },
      "source": [
        "## Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tu9XyRPIUpB"
      },
      "source": [
        "def my_loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "    return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_ZtID7VKPcx"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBFiowVhLik2"
      },
      "source": [
        "@tf.function\n",
        "def train_step(original_seq, target_seq_in, target_seq_out):\n",
        "  with tf.GradientTape() as tape:\n",
        "    padding_mask = 1 - tf.cast(tf.equal(original_seq, 0), dtype=tf.float32)\n",
        "    padding_mask = tf.expand_dims(padding_mask, axis=1)\n",
        "    padding_mask = tf.expand_dims(padding_mask, axis=1)\n",
        "\n",
        "    encoder_output = encoder(original_seq, padding_mask)\n",
        "    decoder_output = decoder(target_seq_in, encoder_output, padding_mask)\n",
        "    loss = my_loss_func(target_seq_out, decoder_output)\n",
        "  \n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  train_acc_metric.update_state(target_seq_out, decoder_output)\n",
        "  return loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmrJNB2YLn1j"
      },
      "source": [
        "## Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yaz1kmhLpjk"
      },
      "source": [
        "def predict(original_text=None):\n",
        "  # If test sentence is not provided randomly pick up one from the training data\n",
        "  if original_text is None:\n",
        "    original_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "  print(\" \" + original_text)\n",
        "\n",
        "  # Tokenize the test sentence to obtain source sequence\n",
        "  original_seq = en_tokenizer.texts_to_sequences([original_text])\n",
        "  \n",
        "  en_output = encoder(tf.constant(original_seq))\n",
        "  de_input = tf.constant([[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "  out_words = []\n",
        "  while True:\n",
        "    de_output = decoder(de_input, en_output)\n",
        "\n",
        "    # Take the last token as the predicted token\n",
        "    new_token = tf.expand_dims(tf.argmax(de_output, -1)[:,-1], axis=1)\n",
        "    out_words.append(fr_tokenizer.index_word[new_token.numpy()[0][0]])\n",
        "\n",
        "    de_input = tf.concat((de_input, new_token), axis=-1)\n",
        "\n",
        "    if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "      break\n",
        "\n",
        "  print(\" \" + ' '.join(out_words))\n",
        "  print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tleqEFGFQNqA"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjvJtWG8efr"
      },
      "source": [
        "def get_training_info(epoch, num_e, batch, len_dataset, accuracy):\n",
        "  epoch_info = \"Epoch:\" + str(epoch + 1) + \"/\" + str(num_e) + \" | \"\n",
        "  num_batch_info = \"Batch:\" + str(batch + 1) + \"/\" + str(len_dataset) + \" | \"\n",
        "  accuracy_info = \"Accuracy:\" + str(accuracy)\n",
        "  training_data = epoch_info + num_batch_info + accuracy_info\n",
        "  time.sleep(1)\n",
        "  return training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "p16rH9FCQO9i",
        "outputId": "0cd6d962-8afd-4a79-c388-719c01ad01c4"
      },
      "source": [
        "NUM_EPOCHS = 1000\n",
        "start_time = time.time()\n",
        "out = display(IPython.display.Pretty('Starting'), display_id=True)\n",
        "\n",
        "print_output = \"Training Model:\"\n",
        "training_info = \"\"\n",
        "for e in range(NUM_EPOCHS):\n",
        "  print_output += training_info + \"\\n\"\n",
        "\n",
        "  for batch, (original_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "    loss = train_step(original_seq, target_seq_in, target_seq_out)\n",
        "\n",
        "    training_info = get_training_info(e, NUM_EPOCHS,  batch, len(dataset), train_acc_metric.result().numpy())\n",
        "    out.update(IPython.display.Pretty(print_output + training_info))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Starting Training:\n",
              "Epoch:1/1000 | Batch:4/4 | Accuracy:0.05357143\n",
              "Epoch:2/1000 | Batch:4/4 | Accuracy:0.0625\n",
              "Epoch:3/1000 | Batch:4/4 | Accuracy:0.065476194\n",
              "Epoch:4/1000 | Batch:4/4 | Accuracy:0.06696428\n",
              "Epoch:5/1000 | Batch:4/4 | Accuracy:0.06928571\n",
              "Epoch:6/1000 | Batch:4/4 | Accuracy:0.073214285\n",
              "Epoch:7/1000 | Batch:4/4 | Accuracy:0.07653061\n",
              "Epoch:8/1000 | Batch:4/4 | Accuracy:0.079017855\n",
              "Epoch:9/1000 | Batch:4/4 | Accuracy:0.08214286\n",
              "Epoch:10/1000 | Batch:4/4 | Accuracy:0.084285714\n",
              "Epoch:11/1000 | Batch:4/4 | Accuracy:0.08668831\n",
              "Epoch:12/1000 | Batch:4/4 | Accuracy:0.08928572\n",
              "Epoch:13/1000 | Batch:4/4 | Accuracy:0.09120879\n",
              "Epoch:14/1000 | Batch:4/4 | Accuracy:0.093367346\n",
              "Epoch:15/1000 | Batch:4/4 | Accuracy:0.09547619\n",
              "Epoch:16/1000 | Batch:4/4 | Accuracy:0.09709822\n",
              "Epoch:17/1000 | Batch:4/4 | Accuracy:0.09894958\n",
              "Epoch:18/1000 | Batch:4/4 | Accuracy:0.102380954\n",
              "Epoch:19/1000 | Batch:4/4 | Accuracy:0.10582707\n",
              "Epoch:20/1000 | Batch:4/4 | Accuracy:0.109107144\n",
              "Epoch:21/1000 | Batch:4/4 | Accuracy:0.11411565\n",
              "Epoch:22/1000 | Batch:4/4 | Accuracy:0.12142857\n",
              "Epoch:23/1000 | Batch:4/4 | Accuracy:0.13043478\n",
              "Epoch:24/1000 | Batch:4/4 | Accuracy:0.13928571\n",
              "Epoch:25/1000 | Batch:4/4 | Accuracy:0.14985715\n",
              "Epoch:26/1000 | Batch:4/4 | Accuracy:0.16112638\n",
              "Epoch:27/1000 | Batch:4/4 | Accuracy:0.17328042\n",
              "Epoch:28/1000 | Batch:4/4 | Accuracy:0.18711735\n",
              "Epoch:29/1000 | Batch:4/4 | Accuracy:0.20012315\n",
              "Epoch:30/1000 | Batch:1/4 | Accuracy:0.2039072"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f51593cb5f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-2806820d40a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moriginal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtraining_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-6c34e7fe95d8>\u001b[0m in \u001b[0;36mget_training_info\u001b[0;34m(epoch, num_e, batch, len_dataset, accuracy)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0maccuracy_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Accuracy:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_info\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_batch_info\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maccuracy_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8F2f5UNoRmh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKx-bzM3foHk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}